{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3, problem 4\n",
    "\n",
    "Finite difference gradient approximation to check the gradient of the cost function with respect to the weights and biases in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1, accuracy rate 0.1866\n",
      "Epoch number 2, accuracy rate 0.2737\n",
      "Epoch number 3, accuracy rate 0.3897\n",
      "Epoch number 4, accuracy rate 0.5041\n",
      "Epoch number 5, accuracy rate 0.5957\n",
      "Epoch number 6, accuracy rate 0.6573\n",
      "Epoch number 7, accuracy rate 0.7102\n",
      "Epoch number 8, accuracy rate 0.7557\n",
      "Epoch number 9, accuracy rate 0.8067\n",
      "Epoch number 10, accuracy rate 0.8395\n",
      "Epoch number 11, accuracy rate 0.8644\n",
      "Epoch number 12, accuracy rate 0.8789\n",
      "Epoch number 13, accuracy rate 0.8842\n",
      "Epoch number 14, accuracy rate 0.8905\n",
      "Epoch number 15, accuracy rate 0.868\n",
      "Epoch number 16, accuracy rate 0.8863\n",
      "Epoch number 17, accuracy rate 0.8968\n",
      "Epoch number 18, accuracy rate 0.8958\n",
      "Epoch number 19, accuracy rate 0.9001\n",
      "Epoch number 20, accuracy rate 0.8882\n",
      "Epoch number 21, accuracy rate 0.8958\n",
      "Epoch number 22, accuracy rate 0.8992\n",
      "Epoch number 23, accuracy rate 0.8948\n",
      "Epoch number 24, accuracy rate 0.9018\n",
      "Epoch number 25, accuracy rate 0.8984\n",
      "Epoch number 26, accuracy rate 0.8956\n",
      "Epoch number 27, accuracy rate 0.8996\n",
      "Epoch number 28, accuracy rate 0.8956\n",
      "Epoch number 29, accuracy rate 0.9007\n",
      "Epoch number 30, accuracy rate 0.8953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reshape_params (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"hw3_q3.jl\")\n",
    "\n",
    "# unroll the weights and biases into a single vector.\n",
    "# note this function will also work for unrolling the gradient.\n",
    "# note that this is hard-coded for a 3-layer NN.\n",
    "function unroll(W, b)\n",
    "    vcat(vec(W[1]), vec(W[2]), vec(b[1]), vec(b[2]))\n",
    "end\n",
    "\n",
    "# given a single vector θ, reshape the parameters into the data\n",
    "# structures that are used for backpropagation, that is, W and b, or\n",
    "# ∇w and ∇b.  note that this is hard-coded for a 3-layer NN.\n",
    "function reshape_params(θ)\n",
    "    n1 = sizes[1]  # number of nodes in layer 1\n",
    "    n2 = sizes[2]  # number of nodes in layer 2\n",
    "    n3 = sizes[3]\n",
    "    W1 = reshape(θ[1:(n2*n1)], n2, n1)\n",
    "    W2 = reshape(θ[(n2*n1 + 1):(n2*n1 + n2*n3)], n3, n2)\n",
    "    b1 = θ[(n2*n1 + n2*n3 + 1):(n2*n1 + n2*n3 + n2)]\n",
    "    b2 = θ[(n2*n1 + n2*n3 + n2 + 1):length(θ)]\n",
    "    W = [ W1, W2 ]\n",
    "    b = [ b1, b2 ]\n",
    "    return W, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "J (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the cost function for a batch of training examples\n",
    "# θ is the unrolled vector of weights and biases.\n",
    "# batch is the set of indices of the batch of training examples.\n",
    "function J(θ, batch, λ)\n",
    "    \n",
    "    m = length(batch)\n",
    "    sumJ = 0.0  # to accumulate the sum for the batch.\n",
    "    # we need to pass W, b to feedforward, so we re-create W, b from θ\n",
    "    W, b = reshape_params(θ)\n",
    "    for i in 1:m\n",
    "        # grab training example i\n",
    "        x_i = trainx[i, :]\n",
    "        y_i = trainy[i]\n",
    "        # feedforward to obtain a, z\n",
    "        (a,z) = feedforward(W, b, x_i)\n",
    "        # accumulate the cost function\n",
    "        sumJ = sumJ + 1/2 * norm(a[3] - digit2vector(y_i))^2\n",
    "    end\n",
    "\n",
    "    # return the cost function. note that the regularization term only\n",
    "    # applies to the weights, not the biases\n",
    "    return 1/m * sumJ + λ/2 * (sum(W[1].^2) + sum(W[2].^2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "θminus (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ith basis vector\n",
    "function e(i)\n",
    "    e = zeros(sizes[2]*sizes[1] + sizes[3]*sizes[2] + sizes[2] + sizes[3])\n",
    "    e[i] = 1\n",
    "    return e\n",
    "end\n",
    "\n",
    "θplus(v, i; ϵ=1e-4) = v .+ ϵ*e(i)\n",
    "θminus(v, i; ϵ=1e-4) = v .- ϵ*e(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compare1 (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the difference between the ith element of the gradient as\n",
    "# computed from backpropagation (this is ∇θ[i]) and the approximation of\n",
    "# the ith element of the gradient as obtained from finite differencing.\n",
    "# the idea is to see if the backpropagation code is correctly computing\n",
    "# the gradient of the cost function.\n",
    "function compare1(i, θ, ∇θ, batch, λ; ϵ=1e-4)\n",
    "    # i is the ith element of the unrolled gradient θ,\n",
    "    return ∇θ[i] - ( J(θplus(θ, i, ϵ=ϵ), batch, λ) - J(θminus(θ, i, ϵ=ϵ), batch, λ) )/(2*ϵ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference exceeding 0.001: -0.0017222017197629736\n",
      "difference exceeding 0.001: 0.09502166425766669\n",
      "difference exceeding 0.001: -0.04651520520531268\n",
      "difference exceeding 0.001: -0.006357655681201752\n",
      "difference exceeding 0.001: 0.057783665731816924\n",
      "difference exceeding 0.001: -0.0035090734959547106\n",
      "difference exceeding 0.001: 0.017294729970658382\n",
      "difference exceeding 0.001: 0.4971653724017286\n",
      "difference exceeding 0.001: 0.0016031548483228962\n",
      "difference exceeding 0.001: 0.001237423307966085\n",
      "difference exceeding 0.001: -0.0020834912100179395\n",
      "difference exceeding 0.001: 0.0010240464342660045\n",
      "difference exceeding 0.001: -0.00151329751356005\n",
      "difference exceeding 0.001: -0.002015308614627713\n",
      "difference exceeding 0.001: -0.0039903406663976786\n",
      "difference exceeding 0.001: -0.01249970254486283\n",
      "difference exceeding 0.001: 0.2955195798302165\n",
      "difference exceeding 0.001: 0.0015268001391765872\n",
      "difference exceeding 0.001: -0.00264758153517917\n",
      "difference exceeding 0.001: 0.0017482259022032801\n",
      "difference exceeding 0.001: -0.0025733004643623517\n",
      "difference exceeding 0.001: -0.004188307938619619\n",
      "difference exceeding 0.001: -0.0039199038319496305\n",
      "difference exceeding 0.001: 0.001045988620847309\n",
      "difference exceeding 0.001: -0.14589802515594857\n",
      "difference exceeding 0.001: 0.002334273740966393\n",
      "difference exceeding 0.001: -0.0032793531415019894\n",
      "difference exceeding 0.001: -0.0030466878934725327\n",
      "difference exceeding 0.001: 0.0021794444778918693\n",
      "difference exceeding 0.001: 0.0012779866786295356\n",
      "difference exceeding 0.001: 0.004218986847099064\n",
      "difference exceeding 0.001: -0.0032793531415019894\n",
      "difference exceeding 0.001: -0.002013177857355855\n",
      "difference exceeding 0.001: -0.0025499834413148154\n",
      "difference exceeding 0.001: 0.0010524102831652611\n",
      "difference exceeding 0.001: -0.001640601444143016\n",
      "difference exceeding 0.001: 0.0090233847760475\n",
      "difference exceeding 0.001: 0.0026265918869555465\n",
      "difference exceeding 0.001: 0.04442922822670151\n",
      "difference exceeding 0.001: 0.001603052281465823\n",
      "difference exceeding 0.001: 0.0025534732390087014\n",
      "difference exceeding 0.001: 0.0020996085044065263\n",
      "difference exceeding 0.001: -0.0014590555885280047\n",
      "difference exceeding 0.001: -0.0012208093718705047\n",
      "difference exceeding 0.001: -0.0023385808863827726\n",
      "difference exceeding 0.001: -0.0014921271864360357\n",
      "difference exceeding 0.001: -0.0010643347298055197\n",
      "difference exceeding 0.001: -0.00958585576431653\n",
      "difference exceeding 0.001: -0.022168642443927643\n",
      "difference exceeding 0.001: 0.001882358480666996\n",
      "difference exceeding 0.001: 0.001110327001998277\n",
      "difference exceeding 0.001: 0.0015971938595034571\n",
      "difference exceeding 0.001: -0.0018585915380212158\n",
      "difference exceeding 0.001: -0.0068846147202529275\n",
      "difference exceeding 0.001: -0.0012325004206451616\n",
      "difference exceeding 0.001: -0.0011787182486494578\n",
      "difference exceeding 0.001: -0.041186644180601056\n",
      "difference exceeding 0.001: -0.0022796787791415168\n",
      "difference exceeding 0.001: -0.04523395801697063\n",
      "difference exceeding 0.001: 0.0010335376252015634\n",
      "difference exceeding 0.001: 0.0024273542356199533\n",
      "difference exceeding 0.001: -0.0015533293952833074\n",
      "difference exceeding 0.001: -0.0017168391792116106\n",
      "Number of components that exceeded the tolerance: 63\n"
     ]
    }
   ],
   "source": [
    "# compare each element of the gradient as computed from\n",
    "# backpropagation to its estimate as obtained from finite\n",
    "# differencing.\n",
    "function compare(W, b, ∇W, ∇b, λ)\n",
    "    θ = unroll(W, b)\n",
    "    ∇θ = unroll(∇W, ∇b)\n",
    "    m = length(trainy)\n",
    "\n",
    "    # create a batch of 5000 training examples to evaluate the cost function.\n",
    "    # we really just need the indices of the batch.\n",
    "    batch = sample(1:m, 5000)\n",
    "\n",
    "    # random sample of 200 gradient components to check\n",
    "    components = sample(1:length(θ), 200)\n",
    "\n",
    "    # loop over the 200 gradient components.\n",
    "    # for each gradient component\n",
    "    #   perform finite differencing by calling compare1\n",
    "    #   if the difference exeeds 0.001\n",
    "    #      print a message\n",
    "    #   end\n",
    "    # print number of components that exceeded the tolerance of 0.001\n",
    "    count = 0\n",
    "    for i in 1:length(components)\n",
    "        difference = compare1(components[i], θ, ∇θ, batch, λ)\n",
    "        if abs(difference) > 0.001\n",
    "            println(\"difference exceeding 0.001: \", difference)\n",
    "            count = count + 1\n",
    "        end\n",
    "    end\n",
    "    println(\"Number of components that exceeded the tolerance: \", count)\n",
    "end\n",
    "\n",
    "# Note: W, b, ∇W, ∇b have been already been\n",
    "# computed. Use your code from problem 3 to do this.\n",
    "# λ should be same as the λ that was used for problem 3.\n",
    "compare(W, b, ∇W, ∇b, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 63 gradient components exceeding the tolerance of 0.001, but the majority of them are not far off 0.001. This means that the backpropagation code computed the gradient of the cost function pretty accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
