{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3, problem 3\n",
    "\n",
    "Neural networks for digits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "digit2vector (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN to recognize hand-written digits using the MNIST data\n",
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read the MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the activation function\n",
    "@. f(z) = 1/(1 + exp(-z))      # sigmoid activation\n",
    "@. fprime(z) = f(z) * (1-f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    return vcat( repeat([0],d), 1, repeat([0],9-d) )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a) Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "function feedforward(W, b, x)\n",
    "    a = [ x, zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    z = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    z[2] = W[1] * x + b[1]\n",
    "    a[2] = f(z[2])\n",
    "    z[3] = W[2] * a[2] + b[2]\n",
    "    a[3] = f(z[3])\n",
    "    \n",
    "    return a, z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classify (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    return argmax(a[3]) - 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_error (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* fprime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* fprime(z[2])\n",
    "\n",
    "    return δ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_gradients (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),\n",
    "           zeros(sizes[3], sizes[2]) ]    # ((30x784), (10x30))\n",
    "    ∇b = [ zeros(sizes[2]), zeros(sizes[3]) ]    # ((30x1), (10x1))\n",
    "    \n",
    "    ∇W[1] = δ[2] * a[1]'    # (30x1) x (1x784)\n",
    "    ∇W[2] = δ[3] * a[2]'    # (10x1) x (1x30)\n",
    "    ∇b[1] = δ[2]    # 30x1\n",
    "    ∇b[2] = δ[3]    # 10x1\n",
    "    \n",
    "    return ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GD (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "    for i in 1:m\n",
    "        (∇W, ∇b) = backprop(W, b, trainx[batch[i], :], trainy[batch[i]])\n",
    "        sumW = sumW + ∇W\n",
    "        sumb = sumb + ∇b\n",
    "    end\n",
    "\n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "    ∇W = (1/m) .* sumW + λ.*W\n",
    "    ∇b = (1/m) .* sumb\n",
    "    W = W - α .* ((1/m) .* sumW + λ.*W)\n",
    "    b = b - α .* ((1/m) .* sumb)\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c) Function to calculate the Classification Accuracy\n",
    "\n",
    "This function calculates the proportion of test samples that are correctly classified after one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b) \n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    return sum(testy .== yhat)/ntest # hit rate\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b) Train the neural network and Calculate the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BGD (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01) \n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "    \n",
    "    for i in 1:epochs\n",
    "        remaining = 1:60000\n",
    "        while length(remaining) > 0\n",
    "            batch = sample(remaining, m)\n",
    "            remaining = setdiff(remaining, batch)\n",
    "            (W, b, ∇W, ∇b) = GD(W, b, batch)\n",
    "        end\n",
    "        accuracy_rate = accuracy(W, b)\n",
    "        println(\"Epoch number \", i, \", accuracy rate \", accuracy_rate)\n",
    "    end\n",
    "    \n",
    "    return W, b, ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1, accuracy rate 0.1356\n",
      "Epoch number 2, accuracy rate 0.1962\n",
      "Epoch number 3, accuracy rate 0.2948\n",
      "Epoch number 4, accuracy rate 0.3953\n",
      "Epoch number 5, accuracy rate 0.5203\n",
      "Epoch number 6, accuracy rate 0.6008\n",
      "Epoch number 7, accuracy rate 0.6691\n",
      "Epoch number 8, accuracy rate 0.7382\n",
      "Epoch number 9, accuracy rate 0.7918\n",
      "Epoch number 10, accuracy rate 0.8473\n",
      "Epoch number 11, accuracy rate 0.8633\n",
      "Epoch number 12, accuracy rate 0.8795\n",
      "Epoch number 13, accuracy rate 0.8881\n",
      "Epoch number 14, accuracy rate 0.8958\n",
      "Epoch number 15, accuracy rate 0.8947\n",
      "Epoch number 16, accuracy rate 0.8997\n",
      "Epoch number 17, accuracy rate 0.8981\n",
      "Epoch number 18, accuracy rate 0.8952\n",
      "Epoch number 19, accuracy rate 0.9013\n",
      "Epoch number 20, accuracy rate 0.8939\n",
      "Epoch number 21, accuracy rate 0.8998\n",
      "Epoch number 22, accuracy rate 0.9003\n",
      "Epoch number 23, accuracy rate 0.9003\n",
      "Epoch number 24, accuracy rate 0.9009\n",
      "Epoch number 25, accuracy rate 0.9023\n",
      "Epoch number 26, accuracy rate 0.8978\n",
      "Epoch number 27, accuracy rate 0.897\n",
      "Epoch number 28, accuracy rate 0.9033\n",
      "Epoch number 29, accuracy rate 0.895\n",
      "Epoch number 30, accuracy rate 0.9026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[-0.00011406305338195666 -0.00014226568993077924 … 6.168292841667688e-5 0.0001190171662232156; -2.8463270715875076e-5 7.398299498850874e-5 … -0.00012146465620489647 7.447742497371777e-5; … ; -1.1099753485309021e-6 -2.2351709674146346e-5 … 0.00021809552203799065 -2.3954551635602565e-5; -9.475144900215776e-5 5.073437780367948e-5 … -0.00024569744038471943 4.579729857995711e-5], [0.024123171853695714 -0.03891809844912488 … -0.09023286459127625 -0.02395364231598385; 0.0033083406294131474 0.015172651472068293 … -0.06476309513965042 0.5207648866277951; … ; -0.3598756572017 0.03891651285489906 … -0.05576568824347911 -0.12743351467699388; 0.015028526586535567 -0.08517013129027273 … -0.19299913148131098 -0.060816438903199904]], [[-0.43067155850212036, -0.06583076615395542, -0.1553442973891528, -1.0745673808163534, 1.3336266234338696, -0.3382831935710984, -0.09083842081333818, 0.3339418195791132, 0.9703927158373492, -0.5026437567248545  …  0.5318940568942931, 0.0838206040531174, -1.4938773861885393, 1.4363835659069737, -0.9528771182254386, -0.058177526340235326, 0.2282292366249262, 0.909980943950436, -1.7961194759191994, -0.542790443431766], [-1.5984449633402567, -1.3699873762470567, -1.1203256977623062, -1.154368271965173, -1.3978487909467001, -1.0618455410363696, -1.412590480018249, -1.2025121970758441, -2.8149824938354753, -0.05726129928050068]], [[-1.1407446082803946e-6 -1.4227991792257148e-6 … 6.168909732640952e-7 1.190290691301286e-6; -2.8466117327607835e-7 7.399039402791152e-7 … -1.21476803885285e-6 7.448487346106388e-7; … ; -1.1100863571666188e-8 -2.2353945068653213e-7 … 2.181173337713678e-6 -2.3956947330335597e-7; -9.476092509466723e-7 5.073945174885436e-7 … -2.4572201258597804e-6 4.580187876783389e-7], [0.0033188307300497 -0.016347886614560967 … 0.0007031921683448043 0.0001792236333266497; -0.008582657741802306 -0.0049037654772203934 … -3.730802412075932e-5 -0.004533637098600879; … ; 0.0003102641538255519 -0.011234419194293625 … 0.00023665706135723118 0.0008365419979456253; -0.01094868574681081 -0.008526682865215114 … 0.00666742480353433 0.0007707844979560276]], [[5.969816116489567e-14, -4.2146942350128886e-14, -0.0011118470851827402, 1.5737707251191682e-46, -1.0272773236926913e-5, 1.691942526947849e-12, 8.538506215252839e-23, 3.6261926860386958e-22, 1.376003712137558e-47, 8.062352072373385e-6  …  2.457910411512031e-5, -0.00012309023916865758, -3.691281070434709e-10, -9.29025654863853e-18, 3.576805698506466e-15, 6.927120963658653e-17, -4.212240357630792e-8, 6.244892626577581e-12, 8.73826426038621e-11, -5.588707996781682e-16], [-0.01595707084139817, -0.005055001615628416, 0.00794526482465225, 0.009072625456111472, 0.010690579336623093, 0.01052090485679737, 0.008861747674093636, -0.00116499580110698, -0.011622460880872244, -0.007674128883992589]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 20       # batch size\n",
    "epochs = 30  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I train the neural networks using gradient descent with backpropagation, the neural networks achieve an accuracy of roughly 90%, meaning that given a set of input data, the networks will classify the digits correctly 90% of the time. The weights and biases are initialized randomly but through gradient descent with backpropagation, the networks \"learn\" and modify those matrices accordingly for better classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
